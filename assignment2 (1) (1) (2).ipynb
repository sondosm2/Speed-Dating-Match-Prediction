{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b703feb",
   "metadata": {},
   "source": [
    "# Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202c580",
   "metadata": {},
   "source": [
    "Data has a lot of missing values and unbalanced .                                                        \n",
    "we need to make data preprocessing to model training as a single pipeline, and search for the hyperparameters for this pipeline. Then we must apply grid search , random and bayesian search to find the best hyperparameter and know what is the best search and the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb30bf",
   "metadata": {},
   "source": [
    "# The input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cb82e",
   "metadata": {},
   "source": [
    "The input contains all features without (Id)\n",
    "\n",
    "The output represents as y (match feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.xolumns its the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e986da6",
   "metadata": {},
   "source": [
    "# What data mining function is required?\n",
    "predict function is used here because you need to know if people matches with each other or not thus i make classifier models to predict it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e70b85",
   "metadata": {},
   "source": [
    "# What could be the challenges?                                                             \n",
    "-find the best classifier with best hyperparameter                                                      \n",
    "-make three ways of search to tune the models                                  \n",
    "-how to choose the best hyperparameter that make the highest f1-score                         \n",
    "-make the best fill for the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c4974",
   "metadata": {},
   "source": [
    "# What is the impact?\n",
    "analyze the performance of the models when trained on each resulting dataset with respect to f1-score.\n",
    "we use grid,random, bayesian search to tune hyperparameters of classifier models this contains Logistic , SVM,decision tree,random forest and Xgboost.\n",
    "After i made grid search for Logistic regression and decision tree , i found logistic regression had f1- score(0.86) more than decision tree (0.82).So the trial 1 for decision has the best accuracy for decision tree.\n",
    "After that i made bayesian search (it's the best search and faster than grid) for logistic had f1-score =0.87 , xgboost had f1-score=0.88 and SVM had 0.87 , i found xgboost had the highest f1-score (0.88). After that i made random search (fast but not the best) for logistic and decision tree , i found logistic classifier had f1-score (0.86)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931dcdb",
   "metadata": {},
   "source": [
    "# What is an ideal solution?\n",
    "ideal best solution is xgboost classifier , it gives high F1-score = 0.88 with ('my_classifier__learning_rate', 0.05), ('my_classifier__max_depth', 8), ('my_classifier__n_estimators', 600) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a8d9d",
   "metadata": {},
   "source": [
    "# What preprocessing steps are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1c8ea",
   "metadata": {},
   "source": [
    "-split data into numeric and category sets                                                 \n",
    "-fill null values with mean                  \n",
    "-choose the best featuers by making feature selection to find the relevant features to use in model construction.                          \n",
    "-make on hot encoding for category columns                                               \n",
    "-make standard scaler for numeric value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e75e8",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8800072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7af5ea",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6ee7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('train1.csv',index_col='id')\n",
    "data_test=pd.read_csv('test1.csv')\n",
    "data_testd=data_test.drop(columns='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71f472",
   "metadata": {},
   "source": [
    "# Data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63963a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.duplicated().sum() # check if there is dublicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5c4793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender         0\n",
       "idg            0\n",
       "condtn         0\n",
       "wave           0\n",
       "round          0\n",
       "            ... \n",
       "attr5_3     4496\n",
       "sinc5_3     4496\n",
       "intel5_3    4496\n",
       "fun5_3      4496\n",
       "amb5_3      4496\n",
       "Length: 191, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isna().sum() #check null value for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19faf622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5909 entries, 2583 to 8149\n",
      "Columns: 191 entries, gender to amb5_3\n",
      "dtypes: float64(173), int64(10), object(8)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info() #print all column headers + type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb29b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5909 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "id                                                                           \n",
       "2583       0    3       2    14     18         2       2.0     14       12   \n",
       "6830       1   14       1     3     10         2       NaN      8        8   \n",
       "4840       1   14       1    13     10         8       8.0     10       10   \n",
       "5508       1   38       2     9     20        18      13.0      6        7   \n",
       "4828       1   24       2    14     20         6       6.0     20       17   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "3390       0    1       2     9     20         2       2.0     18        1   \n",
       "4130       1   24       2     9     20        19      15.0      5        6   \n",
       "1178       0   13       2    11     21         5       5.0      3       18   \n",
       "5016       1   10       2     7     16         6      14.0      9       10   \n",
       "8149       0    7       2    21     22         7       7.0      2       12   \n",
       "\n",
       "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
       "id           ...                                                        \n",
       "2583  372.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "6830   63.0  ...      6.0      8.0       8.0     7.0     8.0      NaN   \n",
       "4840  331.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "5508  200.0  ...      8.0      9.0       8.0     8.0     6.0      NaN   \n",
       "4828  357.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "...     ...  ...      ...      ...       ...     ...     ...      ...   \n",
       "3390  214.0  ...     12.0     12.0      12.0     9.0    12.0      NaN   \n",
       "4130  199.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "1178  290.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "5016  151.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "8149  542.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "\n",
       "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
       "id                                       \n",
       "2583      NaN       NaN     NaN     NaN  \n",
       "6830      NaN       NaN     NaN     NaN  \n",
       "4840      NaN       NaN     NaN     NaN  \n",
       "5508      NaN       NaN     NaN     NaN  \n",
       "4828      NaN       NaN     NaN     NaN  \n",
       "...       ...       ...     ...     ...  \n",
       "3390      NaN       NaN     NaN     NaN  \n",
       "4130      NaN       NaN     NaN     NaN  \n",
       "1178      NaN       NaN     NaN     NaN  \n",
       "5016      NaN       NaN     NaN     NaN  \n",
       "8149      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5909 rows x 191 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b5a2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>-0.004047</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>-0.056275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150992</td>\n",
       "      <td>-0.169387</td>\n",
       "      <td>0.011476</td>\n",
       "      <td>-0.153701</td>\n",
       "      <td>-0.066626</td>\n",
       "      <td>-0.133302</td>\n",
       "      <td>-0.277085</td>\n",
       "      <td>0.080227</td>\n",
       "      <td>-0.065562</td>\n",
       "      <td>0.069091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idg</th>\n",
       "      <td>0.032132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330587</td>\n",
       "      <td>0.093823</td>\n",
       "      <td>0.391918</td>\n",
       "      <td>0.164705</td>\n",
       "      <td>0.174651</td>\n",
       "      <td>0.161976</td>\n",
       "      <td>0.139034</td>\n",
       "      <td>0.088372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>-0.050350</td>\n",
       "      <td>-0.060674</td>\n",
       "      <td>-0.041080</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>-0.018915</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>-0.145645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condtn</th>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.330587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219735</td>\n",
       "      <td>0.820898</td>\n",
       "      <td>0.331013</td>\n",
       "      <td>0.306722</td>\n",
       "      <td>0.331402</td>\n",
       "      <td>0.322467</td>\n",
       "      <td>0.218009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087388</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.061796</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>-0.087502</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.106486</td>\n",
       "      <td>0.123314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wave</th>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.093823</td>\n",
       "      <td>0.219735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228917</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.061166</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.087904</td>\n",
       "      <td>0.996714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>-0.104562</td>\n",
       "      <td>0.053971</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.092556</td>\n",
       "      <td>-0.001751</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.047841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.391918</td>\n",
       "      <td>0.820898</td>\n",
       "      <td>0.228917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380632</td>\n",
       "      <td>0.368352</td>\n",
       "      <td>0.397952</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.218901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094818</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>0.104829</td>\n",
       "      <td>0.092951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attr5_3</th>\n",
       "      <td>-0.133302</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>0.092556</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.201598</td>\n",
       "      <td>0.018610</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858673</td>\n",
       "      <td>0.174329</td>\n",
       "      <td>0.421117</td>\n",
       "      <td>0.410789</td>\n",
       "      <td>0.235485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190198</td>\n",
       "      <td>0.355876</td>\n",
       "      <td>0.412403</td>\n",
       "      <td>0.194078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinc5_3</th>\n",
       "      <td>-0.277085</td>\n",
       "      <td>-0.018915</td>\n",
       "      <td>-0.087502</td>\n",
       "      <td>-0.001751</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>0.111991</td>\n",
       "      <td>0.125594</td>\n",
       "      <td>-0.035897</td>\n",
       "      <td>-0.002601</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224796</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.315857</td>\n",
       "      <td>0.180240</td>\n",
       "      <td>0.122856</td>\n",
       "      <td>0.190198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498313</td>\n",
       "      <td>0.335823</td>\n",
       "      <td>0.261307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intel5_3</th>\n",
       "      <td>0.080227</td>\n",
       "      <td>-0.093206</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>0.089283</td>\n",
       "      <td>0.106791</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335831</td>\n",
       "      <td>0.288062</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.240322</td>\n",
       "      <td>0.268646</td>\n",
       "      <td>0.355876</td>\n",
       "      <td>0.498313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262987</td>\n",
       "      <td>0.422743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun5_3</th>\n",
       "      <td>-0.065562</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>0.106486</td>\n",
       "      <td>0.088716</td>\n",
       "      <td>0.104829</td>\n",
       "      <td>0.135484</td>\n",
       "      <td>0.232412</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.096788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394979</td>\n",
       "      <td>0.160204</td>\n",
       "      <td>0.295997</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.331533</td>\n",
       "      <td>0.412403</td>\n",
       "      <td>0.335823</td>\n",
       "      <td>0.262987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.379768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amb5_3</th>\n",
       "      <td>0.069091</td>\n",
       "      <td>-0.145645</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.047841</td>\n",
       "      <td>0.092951</td>\n",
       "      <td>0.058825</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.043115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194495</td>\n",
       "      <td>0.135499</td>\n",
       "      <td>0.353093</td>\n",
       "      <td>0.429332</td>\n",
       "      <td>0.619087</td>\n",
       "      <td>0.194078</td>\n",
       "      <td>0.261307</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender       idg    condtn      wave     round  position  \\\n",
       "gender    1.000000  0.032132 -0.000875 -0.004192  0.017755 -0.004047   \n",
       "idg       0.032132  1.000000  0.330587  0.093823  0.391918  0.164705   \n",
       "condtn   -0.000875  0.330587  1.000000  0.219735  0.820898  0.331013   \n",
       "wave     -0.004192  0.093823  0.219735  1.000000  0.228917  0.079820   \n",
       "round     0.017755  0.391918  0.820898  0.228917  1.000000  0.380632   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "attr5_3  -0.133302  0.001764  0.077571  0.092556  0.034647  0.066300   \n",
       "sinc5_3  -0.277085 -0.018915 -0.087502 -0.001751 -0.035242  0.111991   \n",
       "intel5_3  0.080227 -0.093206  0.051449  0.014611 -0.012896  0.089283   \n",
       "fun5_3   -0.065562 -0.061079  0.106486  0.088716  0.104829  0.135484   \n",
       "amb5_3    0.069091 -0.145645  0.123314  0.047841  0.092951  0.058825   \n",
       "\n",
       "          positin1     order   partner       pid  ...   attr3_3   sinc3_3  \\\n",
       "gender    0.000410  0.009850  0.010318 -0.056275  ... -0.150992 -0.169387   \n",
       "idg       0.174651  0.161976  0.139034  0.088372  ...  0.011633 -0.050350   \n",
       "condtn    0.306722  0.331402  0.322467  0.218009  ...  0.087388  0.044158   \n",
       "wave      0.061166  0.093478  0.087904  0.996714  ... -0.001678  0.036948   \n",
       "round     0.368352  0.397952  0.390320  0.218901  ...  0.094818  0.036600   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "attr5_3   0.201598  0.018610  0.002868  0.108400  ...  0.858673  0.174329   \n",
       "sinc5_3   0.125594 -0.035897 -0.002601  0.021372  ...  0.224796  0.608213   \n",
       "intel5_3  0.106791 -0.004632  0.003345  0.009585  ...  0.335831  0.288062   \n",
       "fun5_3    0.232412  0.016419  0.031795  0.096788  ...  0.394979  0.160204   \n",
       "amb5_3    0.030543  0.029174  0.050511  0.043115  ...  0.194495  0.135499   \n",
       "\n",
       "          intel3_3    fun3_3    amb3_3   attr5_3   sinc5_3  intel5_3  \\\n",
       "gender    0.011476 -0.153701 -0.066626 -0.133302 -0.277085  0.080227   \n",
       "idg      -0.060674 -0.041080 -0.005502  0.001764 -0.018915 -0.093206   \n",
       "condtn    0.061796  0.069298  0.027447  0.077571 -0.087502  0.051449   \n",
       "wave     -0.104562  0.053971  0.000520  0.092556 -0.001751  0.014611   \n",
       "round     0.022696  0.059530  0.013107  0.034647 -0.035242 -0.012896   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "attr5_3   0.421117  0.410789  0.235485  1.000000  0.190198  0.355876   \n",
       "sinc5_3   0.315857  0.180240  0.122856  0.190198  1.000000  0.498313   \n",
       "intel5_3  0.683871  0.240322  0.268646  0.355876  0.498313  1.000000   \n",
       "fun5_3    0.295997  0.771447  0.331533  0.412403  0.335823  0.262987   \n",
       "amb5_3    0.353093  0.429332  0.619087  0.194078  0.261307  0.422743   \n",
       "\n",
       "            fun5_3    amb5_3  \n",
       "gender   -0.065562  0.069091  \n",
       "idg      -0.061079 -0.145645  \n",
       "condtn    0.106486  0.123314  \n",
       "wave      0.088716  0.047841  \n",
       "round     0.104829  0.092951  \n",
       "...            ...       ...  \n",
       "attr5_3   0.412403  0.194078  \n",
       "sinc5_3   0.335823  0.261307  \n",
       "intel5_3  0.262987  0.422743  \n",
       "fun5_3    1.000000  0.379768  \n",
       "amb5_3    0.379768  1.000000  \n",
       "\n",
       "[183 rows x 183 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the input and output\n",
    "x=data_train.loc[:,data_train.columns!='match']  \n",
    "y=data_train.match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cf185",
   "metadata": {},
   "source": [
    "### split data into train with 80% and test data with 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b3afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e763f",
   "metadata": {},
   "source": [
    "### split dataset into object columns and float+integer and change the datatype from object into category type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07b8ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col=X_train.select_dtypes(include=['object'])\n",
    "for col in object_col.columns.values:\n",
    "    data_train[col] = X_train[col].astype(\"category\")\n",
    "categoric=list(X_train.select_dtypes(include=['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1887a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numeric=list(X_train.select_dtypes(include=['float64', 'int64']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500030f",
   "metadata": {},
   "source": [
    "## make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165a906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# define a pipe line for numeric feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')), #fill the null values with mean\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "# define a pipe line for categorical feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')), # handel missing values\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "        #  is a technique which converts categorical variables to numerical in an interpretable format\n",
    "    ]\n",
    ")\n",
    "# define the preprocessor \n",
    "# we gave them a name so we can set their hyperparameters\n",
    "# we also specify what are the categorical \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, categoric)\n",
    "    ]\n",
    ")\n",
    "# combine the preprocessor with the model as a full tunable pipeline\n",
    "# we gave them a name so we can set their hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246e100",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "-Is a tuning technique that attempts to compute the optimum values of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c8309",
   "metadata": {},
   "source": [
    "#  Trial 1 for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db49ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.83786165        nan 0.83688537 0.83689233 0.83688852\n",
      "        nan 0.84331646        nan 0.83825267 0.8383871  0.83825835\n",
      "        nan 0.84175338        nan 0.84150475 0.84120634 0.84150412\n",
      "        nan 0.5               nan 0.83757812 0.83930636 0.83757812\n",
      "        nan 0.5               nan 0.83058074 0.83166947 0.83057758\n",
      "        nan 0.5               nan 0.82914121 0.82926108 0.82913869\n",
      "        nan 0.83677313        nan 0.83539983 0.83547239 0.83540047\n",
      "        nan 0.84382539        nan 0.83703151 0.83725469 0.83703341\n",
      "        nan 0.84175716        nan 0.84182981 0.84175438 0.84182665\n",
      "        nan 0.5               nan 0.83818992 0.84014313 0.83819055\n",
      "        nan 0.5               nan 0.82961363 0.83096015 0.82960922\n",
      "        nan 0.5               nan 0.82752693 0.82771624 0.82752694\n",
      "        nan 0.83417462        nan 0.83290202 0.8329796  0.83290202\n",
      "        nan 0.84384345        nan 0.83512219 0.83536805 0.83512345\n",
      "        nan 0.84175464        nan 0.84047575 0.84051323 0.84047322\n",
      "        nan 0.5               nan 0.83765051 0.83967145 0.83765367\n",
      "        nan 0.5               nan 0.82817913 0.8298962  0.8281785\n",
      "        nan 0.5               nan 0.82556641 0.82583775 0.82556262\n",
      "        nan 0.83021755        nan 0.826363   0.82656676 0.82642241\n",
      "        nan 0.8451964         nan 0.83175972 0.83215414 0.83175592\n",
      "        nan 0.84176032        nan 0.84054929 0.84031096 0.84054678\n",
      "        nan 0.5               nan 0.8378779  0.83957716 0.83787979\n",
      "        nan 0.5               nan 0.82643075 0.82879338 0.82643644\n",
      "        nan 0.5               nan 0.82215937 0.82259736 0.82215809]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.845196399992787\n",
      "best score {'feat__k': 150, 'my_classifierlog__C': 0.1, 'my_classifierlog__penalty': 'l1', 'my_classifierlog__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "full1_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feat', feature_selection.SelectKBest()),\n",
    "        #is the process of isolating the most consistent, non-redundant, and relevant features to use in model construction\n",
    "        ('my_classifierlog', \n",
    "           LogisticRegression(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "K = [60, 80, 100,150]\n",
    "C = [1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "param_grid = {'feat__k': K,\n",
    "                  'my_classifierlog__C': C,\n",
    "                  'my_classifierlog__penalty': penalty,\n",
    "             'my_classifierlog__solver':['lbfgs', 'liblinear', 'sag']}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "grid_search = GridSearchCV(\n",
    "    full1_pipline, param_grid, cv=2, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4727abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c747a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[969  34]\n",
      " [114  65]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      1003\n",
      "           1       0.66      0.36      0.47       179\n",
      "\n",
      "    accuracy                           0.87      1182\n",
      "   macro avg       0.78      0.66      0.70      1182\n",
      "weighted avg       0.86      0.87      0.86      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,t) ## confusion matrix\n",
    "cr = classification_report(y_test, t,zero_division=1) ## classification report\n",
    "print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca5f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =grid_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionloggrid_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45596d4d",
   "metadata": {},
   "source": [
    "# Trial 1 The Decision Tree model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f3fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=2)]: Done 1796 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=2)]: Done 2446 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=2)]: Done 2800 out of 2800 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8207118121888037\n",
      "best score {'feat__k': 40, 'my_classifierdes__criterion': 'gini', 'my_classifierdes__max_depth': 8, 'my_classifierdes__min_samples_leaf': 100}\n"
     ]
    }
   ],
   "source": [
    "decision_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feat', feature_selection.SelectKBest()),\n",
    "        ('my_classifierdes', \n",
    "        DecisionTreeClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "decision_search={'feat__k':[40, 60, 80, 100,150],\n",
    "    \"my_classifierdes__criterion\" : [\"gini\" , \"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [2 , 4 , 6 , 8, 10, 12],\n",
    "     \"my_classifierdes__min_samples_leaf\": [10,11,20,30]\n",
    " }\n",
    "decision_search = GridSearchCV(\n",
    "    decision_pipline, decision_search, cv=5, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fdf9bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[957  46]\n",
      " [107  72]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93      1003\n",
      "           1       0.61      0.40      0.48       179\n",
      "\n",
      "    accuracy                           0.87      1182\n",
      "   macro avg       0.75      0.68      0.71      1182\n",
      "weighted avg       0.86      0.87      0.86      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predd=decision_search.predict(X_test)\n",
    "cm=confusion_matrix(y_test,predd) ## confusion matrix\n",
    "cr = classification_report(y_test,predd,zero_division=1) ## classification report\n",
    "print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78422fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame()\n",
    "\n",
    "# submission['id'] = data_test['id']\n",
    "\n",
    "# submission['match'] =decision_search.predict_proba(data_testd)[:,1]\n",
    "# submission.to_csv('sample_submissiondecgrid_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5f6a9",
   "metadata": {},
   "source": [
    "## Trail 2 decision tree with diff hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7efc8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=2)]: Done 120 out of 120 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7938815396695926\n",
      "best score {'feat__k': 80, 'my_classifierdes__criterion': 'entropy', 'my_classifierdes__max_depth': 12, 'my_classifierdes__min_samples_leaf': 30}\n"
     ]
    }
   ],
   "source": [
    "decision_search={'feat__k':[80, 100,150],\n",
    "    \"my_classifierdes__criterion\" : [\"entropy\"],\n",
    "     \"my_classifierdes__max_depth\" : [10, 12],\n",
    "     \"my_classifierdes__min_samples_leaf\": [10,11,20,30]\n",
    " }\n",
    "decision_search = GridSearchCV(\n",
    "    decision_pipline, decision_search, cv=5, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =decision_search.predict_proba(data_testd)[:,1]\n",
    "submission.to_csv('sample_submissiondecgrid2_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f626460",
   "metadata": {},
   "source": [
    "# Bayesian Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2d9c1",
   "metadata": {},
   "source": [
    "## Trail 2 for logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61c95fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "best score 0.8418298065981518\n",
      "best score OrderedDict([('feat__k', 80), ('my_classifierlog__C', 0.01), ('my_classifierlog__penalty', 'l2'), ('my_classifierlog__solver', 'sag')])\n"
     ]
    }
   ],
   "source": [
    "#high f1-score for log\n",
    "K = [40, 60, 80, 100,150]\n",
    "C = [0.01, 0.001, 0.0001, 0.00001]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# define ranges for bayes search\n",
    "bayes_search = BayesSearchCV(\n",
    "    full1_pipline,\n",
    "    {'feat__k': K,\n",
    "                  'my_classifierlog__C': C,\n",
    "                  'my_classifierlog__penalty': ['l1', 'l2'],\n",
    "             'my_classifierlog__solver':['liblinear', 'sag']}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=2,scoring='roc_auc',\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80357385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predlogb=bayes_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f03be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm=confusion_matrix(y_test,y_predlogb) ## confusion matrix\n",
    "# cr = classification_report(y_test, y_predlogb,zero_division=1) ## classification report\n",
    "# print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "# print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ef4b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =bayes_search.predict_proba(data_testd)[:,1]\n",
    "submission.to_csv('sample_submissionlogbay_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02b90d",
   "metadata": {},
   "source": [
    "# Trial 1 for xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bcdae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[22:55:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:55:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:55:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   49.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[22:56:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[22:57:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.2s finished\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8652422340794435\n",
      "best score OrderedDict([('my_classifier__learning_rate', 0.05), ('my_classifier__max_depth', 8), ('my_classifier__n_estimators', 600)])\n"
     ]
    }
   ],
   "source": [
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "       ('feat', feature_selection.SelectKBest()),\n",
    "        ('my_classifier', \n",
    "           XGBClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {'feat__k':[40, 60, 80, 100,150],\n",
    "        'my_classifier__max_depth':[1,4,5,7,8],\n",
    "    'my_classifier__n_estimators': [100, 200,300, 400,500,600],\n",
    "    'my_classifier__learning_rate': [0.1, 0.01, 0.05]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "184d1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =bayes_search.predict_proba(data_testd)[:,1]\n",
    "submission.to_csv('sample_submissionxgbb_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c7af6",
   "metadata": {},
   "source": [
    "## Trial 2 without making search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84c8dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:42:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Confusion matrix\n",
      "\n",
      " [[959  44]\n",
      " [ 95  84]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      1003\n",
      "           1       0.66      0.47      0.55       179\n",
      "\n",
      "    accuracy                           0.88      1182\n",
      "   macro avg       0.78      0.71      0.74      1182\n",
      "weighted avg       0.87      0.88      0.87      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without tuning\n",
    "full_pipline = full_pipline.fit(X_train, y_train)\n",
    "po=full_pipline.predict(X_test)\n",
    "cm=confusion_matrix(y_test,po) ## confusion matrix\n",
    "cr = classification_report(y_test, po,zero_division=1) ## classification report\n",
    "print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "print(cr)\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =full_pipline.predict_proba(data_testd)[:,1]\n",
    "# submission.to_csv('sample_submissionxg_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10818fd0",
   "metadata": {},
   "source": [
    "## Trail 3 for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb9b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "[19:09:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "[19:09:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "[19:09:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "[19:09:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:10:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:10:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:10:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:10:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8676018157386669\n",
      "best score OrderedDict([('feat__k', 180), ('my_classifier__learning_rate', 0.008949837496427762), ('my_classifier__max_depth', 4), ('my_classifier__n_estimators', 800)])\n"
     ]
    }
   ],
   "source": [
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "       ('feat', feature_selection.SelectKBest()),\n",
    "        ('my_classifier', \n",
    "           XGBClassifier(),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {'feat__k':[80,140,180],\n",
    "        'my_classifier__max_depth':[1,4,5,7,8],\n",
    "    'my_classifier__n_estimators': [600,700,800],\n",
    "    'my_classifier__learning_rate': [0.001,0.01]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=4,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=4, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "839202a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[981  22]\n",
      " [105  74]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      1003\n",
      "           1       0.77      0.41      0.54       179\n",
      "\n",
      "    accuracy                           0.89      1182\n",
      "   macro avg       0.84      0.70      0.74      1182\n",
      "weighted avg       0.88      0.89      0.88      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po=bayes_search.predict(X_test)\n",
    "cm=confusion_matrix(y_test,po) ## confusion matrix\n",
    "cr = classification_report(y_test, po,zero_division=1) ## classification report\n",
    "print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "print(cr)\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =bayes_search.predict_proba(data_testd)[:,1]\n",
    "submission.to_csv('sample_submissionkkkkkk_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc140d6",
   "metadata": {},
   "source": [
    "# Trial 1 RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78425647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
      "best score 0.8340018462455399\n",
      "best score {'feat__k': 40, 'my_classifierran__max_depth': 10, 'my_classifierran__max_features': 'auto', 'my_classifierran__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "rand_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "         ('feat', feature_selection.SelectKBest()),\n",
    "        ('my_classifierran', \n",
    "           RandomForestClassifier(random_state=42),\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")\n",
    "random_grid = {\n",
    "   'feat__k':[40, 60, 80, 100,150],\n",
    "    \n",
    "    'my_classifierran__max_features': ['auto'],\n",
    "    'my_classifierran__n_estimators': [10, 20],  \n",
    "     # my_classifier__n_estimators points to my_classifier->n_estimators \n",
    "    'my_classifierran__max_depth':[10, 20, 30], \n",
    "\n",
    "}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "random_search = GridSearchCV(\n",
    "    rand_pipline, random_grid, cv=2, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =random_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionrandom_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32b12a",
   "metadata": {},
   "source": [
    "## Trial 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8c548c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[973  30]\n",
      " [119  60]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      1003\n",
      "           1       0.67      0.34      0.45       179\n",
      "\n",
      "    accuracy                           0.87      1182\n",
      "   macro avg       0.78      0.65      0.69      1182\n",
      "weighted avg       0.86      0.87      0.86      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#without tuning \n",
    "rand_pipline.fit(X_train,y_train)\n",
    "py=rand_pipline.predict(X_test)\n",
    "cm=confusion_matrix(y_test,py) ## confusion matrix\n",
    "cr = classification_report(y_test, py,zero_division=1) ## classification report\n",
    "print(\"Confusion matrix\\n\\n\" ,cm , \"\\n\")\n",
    "print(cr)\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =rand_pipline.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionrandomp_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8c27b",
   "metadata": {},
   "source": [
    "## Trail 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a51c0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 162 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.82786663 0.83371679 0.82712765 0.83206553 0.82695885 0.83153877\n",
      " 0.8306511  0.83627567 0.83660384 0.8385221  0.83194897 0.83767528\n",
      " 0.83612566 0.83888663 0.83612566 0.83888663 0.83486523 0.84126935\n",
      " 0.81897846 0.82536514 0.82096043 0.82806669 0.83143456 0.83565431\n",
      " 0.82783764 0.83291894 0.83398169 0.83668941 0.83097918 0.835744\n",
      " 0.82935628 0.83633076 0.82935628 0.83633076 0.83398127 0.83846393\n",
      " 0.81789331 0.82409972 0.82339115 0.82973585 0.83129843 0.83536694\n",
      " 0.82808939 0.83350274 0.83434148 0.83693946 0.83096023 0.83568145\n",
      " 0.82935628 0.83599467 0.82935628 0.83599467 0.83374625 0.83827251\n",
      " 0.82347431 0.83008282 0.83020308 0.8343112  0.82948962 0.83635035\n",
      " 0.83118775 0.83728875 0.82472411 0.83215766 0.83127212 0.83726252\n",
      " 0.83354346 0.8378182  0.83354346 0.8378182  0.829005   0.83511859\n",
      " 0.81902924 0.82412415 0.82063687 0.82839911 0.82273288 0.83014088\n",
      " 0.81779458 0.82563123 0.82398441 0.83106472 0.83124531 0.83676798\n",
      " 0.83509644 0.84001023 0.83509644 0.84001023 0.82686324 0.83214151\n",
      " 0.81901341 0.82554887 0.82094482 0.82772936 0.82232919 0.83039476\n",
      " 0.81669952 0.82516832 0.82048724 0.82828789 0.83141059 0.83672003\n",
      " 0.83387714 0.83902405 0.83387714 0.83902405 0.82645512 0.83187617\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.841269345441719\n",
      "best score {'feat__k': 100, 'my_classifierran__max_depth': 10, 'my_classifierran__max_features': 'sqrt', 'my_classifierran__min_samples_leaf': 4, 'my_classifierran__min_samples_split': 10, 'my_classifierran__n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "random_grid = {\n",
    "   'feat__k':[ 100,150,500],\n",
    "    \n",
    "    'my_classifierran__max_features': ['sqrt'],\n",
    "    'my_classifierran__n_estimators': [30,40],  \n",
    "     # my_classifier__n_estimators points to my_classifier->n_estimators \n",
    "    'my_classifierran__max_depth':[10, 20, 30], \n",
    "    'my_classifierran__min_samples_split': [2, 5, 10], \n",
    "    'my_classifierran__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "random_search = GridSearchCV(\n",
    "    rand_pipline, random_grid, cv=2, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =random_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionrandom_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a529d2f",
   "metadata": {},
   "source": [
    "# SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c599ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.84175867199123\n",
      "best score OrderedDict([('feat__k', 80), ('my_svc__C', 129.7042200576221), ('my_svc__degree', 5), ('my_svc__gamma', 0.033632533813774734), ('my_svc__kernel', 'rbf')])\n"
     ]
    }
   ],
   "source": [
    "SVC_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feat', feature_selection.SelectKBest()),\n",
    "        ('my_svc', SVC(class_weight='balanced'))\n",
    "    ]\n",
    ")\n",
    "# SVC has a class_weight attribute for unbalanced data\n",
    "\n",
    "\n",
    "# define ranges for bayes search\n",
    "bayes_search = BayesSearchCV(\n",
    "    SVC_pipline,\n",
    "    {\n",
    "        'feat__k':[40, 60, 80, 100,150],\n",
    "        'my_svc__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'my_svc__gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'my_svc__degree': Integer(1,8),\n",
    "        'my_svc__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "    },\n",
    "    # number of trials \n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =bayes_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionrandom_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd001d",
   "metadata": {},
   "source": [
    "# Random Search \n",
    "-sets up a grid of hyperparameter values and selects random combinations to train the model and score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e4b5f",
   "metadata": {},
   "source": [
    "## Trial 3 Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08b4ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8530635484781733\n",
      "best score {'my_classifierlog__solver': 'lbfgs', 'my_classifierlog__penalty': 'l2', 'my_classifierlog__C': 0.1, 'feat__k': 150}\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    full1_pipline, param_grid, cv=5, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] =random_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissionrandom_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc1dca",
   "metadata": {},
   "source": [
    "### decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0441cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=2)]: Done 320 out of 320 | elapsed:   30.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8266533182616241\n",
      "best score {'my_classifierdes__criterion': 'entropy', 'my_classifierdes__max_depth': 6, 'my_classifierdes__min_samples_leaf': 30}\n"
     ]
    }
   ],
   "source": [
    "decision_search==RandomizedSearchCV(\n",
    "    decision_pipline, decision_search, cv=5, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(X_train, y_train)\n",
    "\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3db709fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['match'] =decision_search.predict_proba(data_testd)[:,1]\n",
    "#submission.to_csv('sample_submissiondec11_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683b7a4",
   "metadata": {},
   "source": [
    "## Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c08d41",
   "metadata": {},
   "source": [
    "Linear regression deals with continuous values but classification problems deals with discrete values and logistic is type of classification thus ,it deals with discreate values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fbb1d",
   "metadata": {},
   "source": [
    "## What's a decision tree and how it is different to a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a238a",
   "metadata": {},
   "source": [
    "Decision Tree is a Supervised learning technique is used for both classification and Regression problems, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec21f8",
   "metadata": {},
   "source": [
    "-Logistic Regression assumes that the data is linearly (or curvy linearly) separable in space.                   \n",
    "-Decision Trees are non-linear classifiers,data doesn't need to belinearly separable.                            \n",
    "In data type Categorical data works well with decision Trees, while continuous data work well with Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd84f0",
   "metadata": {},
   "source": [
    "## What's the difference between grid search and random search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e984709",
   "metadata": {},
   "source": [
    "-Random Search reaches a very good combination very fast, but it doesn’t reach to give the best hyperparameters combination.\n",
    "\n",
    "-grid search gives the best combination but it can take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2875f",
   "metadata": {},
   "source": [
    "## What's the difference between bayesian search and random search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ace1b",
   "metadata": {},
   "source": [
    "bayesian search usually takes a lot of time and fastes more than grid search, it returns the best hyperparameter while random search is faster than bayesian search but it doesn't return the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
